{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея бустинга - будем обучать алгоритмы в композиции не независимо, а последовательно, так, чтобы каждый следующий старался исправить ошибки построенной композиции. Начальное предсказание можно просто инициализировать константой. Далее, пусть $N - 1$ алгоритмов в композиции уже построены:\n",
    "\n",
    "$$\n",
    "a_{N - 1}(x) = \\sum\\limits_{n = 1}^{N - 1}b_n(x)\n",
    "$$\n",
    "\n",
    "Следующий алгоритм $b_N(x)$ получается из следующей задачи оптимизации:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\ell}\\sum\\limits_{i=1}^{\\ell}\\left(b_N(x_i) - (y_i - a_{N - 1}(x_i))\\right)^2 \\to \\min\\limits_{b_N}\n",
    "$$\n",
    "\n",
    "Таким образом, если удастся решить эту задачу идеально, то для каждого объекта $x_i$ будет верно:\n",
    "\n",
    "$$\n",
    "b_N(x_i) - (y_i - a_{N - 1}(x_i)) = 0 \\Rightarrow y_i = a_{N - 1}(x_i) + b_N(x_i) = a_N(x_i)\n",
    "$$\n",
    "\n",
    "**Общий вид.** Пусть $L$ - функция потерь. Тогда общая задача оптимизации выглядит следующим образом:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\ell}\\sum\\limits_{i=1}^{\\ell}L\\left(y_i, a(x_i)\\right) \\to \\min_a,\n",
    "$$\n",
    "\n",
    "где $a$ - это взвешенная комбинация базовых моделей:\n",
    "\n",
    "$$\n",
    "a(x_i) = \\sum\\limits_n\\gamma_nb_n(x_i)\n",
    "$$\n",
    "\n",
    "Далее, пусть $N - 1$ алгоритмов в композиции уже построены:\n",
    "\n",
    "$$\n",
    "a_{N - 1}(x) = \\sum\\limits_{n = 1}^{N - 1}\\gamma_nb_n(x)\n",
    "$$\n",
    "\n",
    "Следующий алгоритм $b_N(x)$ получается из следующей задачи оптимизации:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\ell}\\sum\\limits_{i=1}^{\\ell}L\\left(y_i, a_{N - 1}(x_i) + \\gamma_Nb_N(x_i)\\right) \\to \\min\\limits_{\\gamma_N, b_N}\n",
    "$$\n",
    "\n",
    "Таким образом, для каждого $x_i$ мы хотим найти $b_N(x_i)$, минимизирующий $L\\left(y_i, a_{N - 1}(x_i) + b_N(x_i)\\right)$, а также оптимальную длину шага. Для этого давайте проделаем что-то вроде градиентного спуска в пространстве алгоритмов:\n",
    "\n",
    "$$\n",
    "b_N(x_i) = -\\frac{\\partial L(y_i, z)}{\\partial z}\\Bigl|_{z = a_{N - 1}(x_i)}\n",
    "$$\n",
    "\n",
    "**Упражнение.** Какую задачу решает базовый алгоритм $b_n(x)$?\n",
    "\n",
    "Если подставить полученное значение $b_N(x_i)$ в задачу оптимизации, то можно заметить, что мы действительно \"сдвинули\" значение функции потерь в сторону наискорейшего убывания по $a_{N - 1}(x_i)$. Следовательно, градиентный бустинг можно рассматривать как процесс градиентного спуска в пространстве алгоритмов.\n",
    "\n",
    "Итак, очередной базовый алгоритм $b_N(x)$ можно обучить предсказывать $-\\frac{\\partial L(y_i, z)}{\\partial z}\\Bigl|_{z = a_{N - 1}(x)}$. В свою очередь, после этого уже можно получить оптимальный шаг:\n",
    "\n",
    "$$\n",
    "\\gamma_N = \\mathrm{arg}\\min_\\gamma \\frac{1}{\\ell}\\sum\\limits_{i=1}^{\\ell}L\\left(y_i, a_{N - 1}(x_i) + \\gamma b_N(x_i)\\right)\n",
    "$$\n",
    "\n",
    "В случае бэггинга все базовые алгоритмы настраиваются на различные выборки из одного и того же распределения на $\\mathbb{X} \\times \\mathbb{Y}$. При этом некоторые из них могут оказаться переобученными, однако усреднение позволяет ослабить этот эффект (объясняется тем, что для некоррелированных алгоритмов разброс композиции оказывается в $N$ раз меньше разброса отдельных алгоритмов, т.е. много деревьев с меньшей вероятностью настроятся на некоторый нетипичный объект по сравнению с одним деревом). Если $N$ достаточно велико, то последующие добавления новых алгоритмов уже не позволят улучшить качество модели.\n",
    "\n",
    "В случае же бустинга каждый алгоритм настраивается на ошибки всех предыдущих, это позволяет на каждом шаге настраиваться на исходное распределение все точнее и точнее. Однако при достаточно большом $N$ это является источником переобучения, поскольку последующие добавления новых алгоритмов будут продолжать настраиваться на обучающую выборку, уменьшая ошибку на ней, при этом уменьшая обобщающую способность итоговой композиции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.linspace(0, 1, 100)\n",
    "X_test = np.linspace(0, 1, 1000)\n",
    "\n",
    "Y_train = (X_train > 0.5).astype(int) + np.random.randn(len(X_train)) * 0.1\n",
    "\n",
    "plt.figure(figsize = (16, 9))\n",
    "plt.scatter(X_train, Y_train, s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor\n",
    "\n",
    "reg = BaggingRegressor(DecisionTreeRegressor(max_depth=2), warm_start=True)\n",
    "plt.figure(figsize=(20, 30))\n",
    "sizes = [1, 2, 5, 20, 100, 500, 1000, 2000]\n",
    "for i, s in enumerate(sizes):\n",
    "    reg.n_estimators = s\n",
    "    reg.fit(X_train.reshape(-1, 1), Y_train)\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.scatter(X_train, Y_train, s=30)\n",
    "    plt.plot(X_test, reg.predict(X_test.reshape(-1, 1)), c='green', linewidth=4)\n",
    "    plt.title('{} trees'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что с некоторого момента итоговая функция перестает меняться с ростом количества деревьев.\n",
    "\n",
    "Теперь проделаем то же самое для градиентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(max_depth=1, learning_rate=1, warm_start=True)\n",
    "plt.figure(figsize=(20, 30))\n",
    "sizes = [1, 2, 5, 20, 100, 500, 1000, 2000]\n",
    "for i, s in enumerate(sizes):\n",
    "    reg.n_estimators = s\n",
    "    reg.fit(X_train.reshape(-1, 1), Y_train)\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.scatter(X_train, Y_train, s=30)\n",
    "    plt.plot(X_test, reg.predict(X_test.reshape(-1, 1)), c='green', linewidth=4)\n",
    "    plt.title('{} trees'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный бустинг довольно быстро построил истинную зависимость, после чего начал настраиваться уже на конкретные объекты обучающей выборки, из-за чего сильно переобучился.\n",
    "\n",
    "\n",
    "Бороться с этой проблемой можно с помощью выбора очень простого базового алгоритма или\n",
    "же искусственным снижением веса новых алгоритмов при помощи шага $\\eta$:\n",
    "$$a_N(x) = \\sum_{n=1}^N \\eta \\gamma_N b_n(x).$$\n",
    "\n",
    "Такая поправка замедляет обучение по сравнению с бэггингом, но зато позволяет получить менее переобученный алгоритм. Тем не менее, важно понимать, что переобучение всё равно будет иметь место при обучении сколь угодно большого количества базовых алгоритмов для фиксированного $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(max_depth=1, learning_rate=0.1, warm_start=True)\n",
    "plt.figure(figsize=(20, 30))\n",
    "sizes = [1, 2, 5, 20, 100, 500, 1000, 2000]\n",
    "for i, s in enumerate(sizes):\n",
    "    reg.n_estimators = s\n",
    "    reg.fit(X_train.reshape(-1, 1), Y_train)\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.xlim([0, 1])\n",
    "    plt.scatter(X_train, Y_train, s=30)\n",
    "    plt.plot(X_test, reg.predict(X_test.reshape(-1, 1)), c='green', linewidth=4)\n",
    "    plt.title('{} trees'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим описанный выше эффект на реальных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds = datasets.load_diabetes()\n",
    "X = ds.data\n",
    "Y = ds.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.5, test_size=0.5)\n",
    "\n",
    "MAX_ESTIMATORS = 300\n",
    "\n",
    "gbclf = BaggingRegressor(warm_start=True)\n",
    "err_train_bag = []\n",
    "err_test_bag = []\n",
    "for i in range(1, MAX_ESTIMATORS+1):\n",
    "    gbclf.n_estimators = i\n",
    "    gbclf.fit(X_train, Y_train)\n",
    "    err_train_bag.append(mean_squared_error(Y_train, gbclf.predict(X_train)))\n",
    "    err_test_bag.append(mean_squared_error(Y_test, gbclf.predict(X_test)))\n",
    "    \n",
    "gbclf = GradientBoostingRegressor(warm_start=True, max_depth=2, learning_rate=0.1)\n",
    "err_train_gb = []\n",
    "err_test_gb = []\n",
    "for i in range(1, MAX_ESTIMATORS+1):\n",
    "    gbclf.n_estimators = i\n",
    "    gbclf.fit(X_train, Y_train)\n",
    "    err_train_gb.append(mean_squared_error(Y_train, gbclf.predict(X_train)))\n",
    "    err_test_gb.append(mean_squared_error(Y_test, gbclf.predict(X_test)))\n",
    "    \n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(err_train_gb, label='GB')\n",
    "plt.plot(err_train_bag, label='Bagging')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('Train')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(err_test_gb, label='GB')\n",
    "plt.plot(err_test_bag, label='Bagging')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('Test')\n",
    "plt.gcf().set_size_inches(15,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный бустинг и случайные леса\n",
    "\n",
    "Сравним поведение двух методов построения композиции алгоритмов над деревьями на примере задачи [Kaggle: Predicting a Biological Response](https://www.kaggle.com/c/bioresponse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "data = pd.read_csv('train_bio.csv')\n",
    "X = data.iloc[:, 1:].values\n",
    "y = data.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbm = GradientBoostingClassifier(n_estimators=250, learning_rate=0.2, verbose=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for learning_rate in [1, 0.5, 0.3, 0.2, 0.1]:\n",
    "\n",
    "    gbm = GradientBoostingClassifier(n_estimators=150, learning_rate=learning_rate, random_state=241).fit(X_train, y_train)\n",
    "    \n",
    "    l = roc_auc_score\n",
    "\n",
    "    test_deviance = np.zeros((gbm.n_estimators,), dtype=np.float64)\n",
    "    for i, y_pred in enumerate(gbm.staged_decision_function(X_test)):\n",
    "        y_pred = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "        test_deviance[i] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    train_deviance = np.zeros((gbm.n_estimators,), dtype=np.float64)\n",
    "    for i, y_pred in enumerate(gbm.staged_decision_function(X_train)):\n",
    "        y_pred = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "        train_deviance[i] = roc_auc_score(y_train, y_pred)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(test_deviance, 'r', linewidth=2)\n",
    "    plt.plot(train_deviance, 'g', linewidth=2)\n",
    "    plt.legend(['test', 'train'])\n",
    "    \n",
    "    plt.title('GBM eta=%.1f, test roc-auc=%.3f, best_est=%d' % (learning_rate, test_deviance.max(), test_deviance.argmax()+1))\n",
    "    plt.xlabel('Number of trees')\n",
    "    plt.ylabel('Metric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого, лучшая композиция построена при $\\eta = 0.1$, включает 24 базовых алгоритма и достигает значения 0.817 на контрольной выборке. При этом случайный лес с таким же количеством базовых алгоритмов уступает градиентному бустингу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=24, random_state=0).fit(X_train, y_train)\n",
    "print ('Train RF ROC-AUC =', roc_auc_score(y_train, rf.predict_proba(X_train)[:,1]))\n",
    "print ('Test RF ROC-AUC = ', roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим также, что при всём этом случайный лес, в отличие от градиентного бустинга, использует глубокие деревья, требующие вычислительных мощностей для их обучения.\n",
    "\n",
    "Для достижения такого же качества случайному лесу требуется гораздо большее число базовых алгоритмов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_estimators in range(10, 101, 10):\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, n_jobs=4).fit(X_train, y_train)\n",
    "    print (n_estimators, 'trees: train ROC-AUC =',  roc_auc_score(y_train, rf.predict_proba(X_train)[:,1]), 'test ROC-AUC =',  roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напоследок можно посмотреть [визуализацию](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html) градиентного бустинга для решающих деревьев различной глубины для функций различного вида."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделяющие поверхности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # fit the estimator\n",
    "    model.fit(X, y)\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap=cmap,\n",
    "                           zorder=1)\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=1000, centers=4,\n",
    "                  random_state=0, cluster_std=1.5)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_r = DecisionTreeClassifier(max_depth=10)\n",
    "plt.figure(figsize=(20,10))\n",
    "visualize_classifier(clf_r, X, y, ax=None, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_r = RandomForestClassifier(n_estimators=1000, max_depth=10)\n",
    "plt.figure(figsize=(20,10))\n",
    "visualize_classifier(clf_r, X, y, ax=None, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf_r = GradientBoostingClassifier(n_estimators=1000, learning_rate=1e-2, max_depth=3)\n",
    "plt.figure(figsize=(20,10))\n",
    "visualize_classifier(clf_r, X, y, ax=None, cmap='rainbow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
